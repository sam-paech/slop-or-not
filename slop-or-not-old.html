<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width,initial-scale=1"/>
<title>Writing Metrics: Slop • Repetition • Top n-grams</title>
<style>
  :root { --bg:#0f1115; --fg:#e6e6e6; --muted:#9aa4ad; --card:#171a21; --accent:#6aa0ff; }
  * { box-sizing: border-box; }
  body { margin: 0; padding: 24px; background: var(--bg); color: var(--fg); font: 14px/1.45 ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, "Helvetica Neue", Arial; }
  h1 { font-size: 18px; margin: 0 0 12px; }
  .row { display: grid; grid-template-columns: 1fr 380px; gap: 18px; }
  textarea { width: 100%; height: 50vh; padding: 12px; background: var(--card); color: var(--fg); border: 1px solid #2a2f3a; border-radius: 8px; resize: vertical; }
  button { background: var(--accent); color: #0b0f16; border: 0; border-radius: 8px; padding: 10px 14px; font-weight: 600; cursor: pointer; }
  button:disabled { opacity: .55; cursor: not-allowed; }
  .panel { background: var(--card); border: 1px solid #2a2f3a; border-radius: 8px; padding: 12px 14px; }
  .muted { color: var(--muted); }
  .metric { display: grid; grid-template-columns: 1fr auto; gap: 8px; padding: 6px 0; border-bottom: 1px dashed #2a2f3a; }
  .metric:last-child { border-bottom: 0; }
  .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace; }
  .section { margin-top: 16px; }
  .pill { display: inline-block; background: #10141c; border: 1px solid #2a2f3a; border-radius: 999px; padding: 4px 8px; margin: 4px 6px 0 0; }
  .pill b { color: #b8c7ff; }
  details { margin-top: 10px; }
  summary { cursor: pointer; color: #c7d5ff; }
  code { background: #10141c; padding: 2px 6px; border-radius: 6px; }
</style>
</head>
<body>
  <h1>Writing Metrics</h1>
  <div class="row">
    <div>
      <textarea id="input" placeholder="Paste or type your long-form text here…"></textarea>
      <div class="section">
        <button id="analyzeBtn" disabled>Analyze</button>
        <span id="status" class="muted" style="margin-left:10px;">loading resources…</span>
      </div>
      <div id="errors" class="section mono" style="white-space:pre-wrap;color:#ffb4b4;"></div>
    </div>
    <div class="panel">
      <div class="metric"><div>Total characters</div><div id="m_chars" class="mono">—</div></div>
      <div class="metric"><div>Total words</div><div id="m_words" class="mono">—</div></div>
      <div class="metric"><div>Slop index (per 1k words)</div><div id="m_slop" class="mono">—</div></div>
      <div class="metric"><div>Repetition score (%)</div><div id="m_rep" class="mono">—</div></div>

      <div class="section">
        <details open>
          <summary>Top over-represented words</summary>
          <div id="topWords" style="margin-top:8px;"></div>
        </details>
        <details open>
          <summary>Top bigrams (over-use vs human)</summary>
          <div id="topBigrams" style="margin-top:8px;"></div>
        </details>
        <details open>
          <summary>Top trigrams (over-use vs human)</summary>
          <div id="topTrigrams" style="margin-top:8px;"></div>
        </details>
        <details>
          <summary>Exact repeated phrases (trigram-based)</summary>
          <div id="topPhrases" style="margin-top:8px;"></div>
        </details>
      </div>
      <div class="section muted" style="font-size:12px;">
        Resources expected alongside this file:<br/>
        <code>human_writing_profile.json</code>, <code>slop_list.json</code>, <code>slop_list_bigrams.json</code>, <code>slop_list_trigrams.json</code>.
      </div>
    </div>
  </div>

<script type="module">
/*
  Implements:
    • Slop index (word + bigram + trigram lists; weights 1,2,8) normalized per 1k words
    • Repetition score using SUBTLEX Zipf baseline
    • Top words by over-representation vs Zipf
    • Top bigrams/trigrams over-used vs human baseline (from human_writing_profile.json)
    • Exact repeated phrases mined from top trigrams

  Assumptions:
    • human_writing_profile.json is in the same directory
    • slop_list*.json are in the same directory; each file is an array like: [["word"], ["two words"], …]
    • SUBTLEX baseline via jsdelivr (fallback supported if structure differs)
*/

const ui = {
  btn: document.getElementById('analyzeBtn'),
  status: document.getElementById('status'),
  errors: document.getElementById('errors'),
  input: document.getElementById('input'),
  m_chars: document.getElementById('m_chars'),
  m_words: document.getElementById('m_words'),
  m_slop: document.getElementById('m_slop'),
  m_rep: document.getElementById('m_rep'),
  topWords: document.getElementById('topWords'),
  topBigrams: document.getElementById('topBigrams'),
  topTrigrams: document.getElementById('topTrigrams'),
  topPhrases: document.getElementById('topPhrases'),
};

const STOPWORDS = new Set([
  "a","an","the","and","or","but","if","then","than","as","of","to","in","on","for",
  "with","by","at","from","that","this","these","those","is","am","are","was","were",
  "be","been","being","it","its","it's","i","you","he","she","they","we","me","him",
  "her","them","us","my","your","his","their","our","yours","hers","theirs","ours",
  "not","no","so","do","does","did","doing","have","has","had","having","will","would",
  "can","could","should","may","might","must","there","here","up","down","out","over",
  "under","again","further","then","once","about","into","through","during","before",
  "after","above","below","between","own","same","other","very","just"
]);

// Function words where cliticizing the aux is not informative for over-rep
const FUNCTION_WORDS = new Set([
  "i","you","he","she","it","we","they","me","him","her","us","them",
  "this","that","these","those","there","here","who","whom","whose","which",
  "what","when","where","why","how"
]);

function lookupZipf(word) {
  // 1) exact
  let z = zipfMap.get(word);
  if (z) return z;

  // 2) possessive strip: dog's -> dog
  if (word.endsWith("'s")) {
    const base = word.slice(0, -2);
    z = zipfMap.get(base);
    if (z) return z;
  }

  // 3) decontract common clitics
  //    If base is a function word (who/what/etc.), skip to avoid "who'd" blowups.
  const m = word.match(/^([a-z]+)('(re|ve|ll|d|m))$/) || word.match(/^([a-z]+)(n't)$/i);
  if (m) {
    const base = m[1];
    if (FUNCTION_WORDS.has(base)) return null; // e.g., who'd, what're → skip

    const clitic = m[2].toLowerCase();
    // Try base alone
    z = zipfMap.get(base);
    if (z) return z;

    // Try expanded auxiliary (approximate; only for non-function bases)
    const auxMap = { "n't":"not", "'re":"are", "'ve":"have", "'ll":"will", "'d":"would", "'m":"am" };
    const aux = auxMap[clitic];
    if (aux) {
      // prefer the content word’s Zipf; fallback to aux if content missing
      z = zipfMap.get(aux);
      if (z) return z;
    }
  }

  // 4) apostrophe-stripped fallback: city’s -> city, don’t -> dont (usually absent)
  const noApos = word.replace(/'/g, "");
  if (noApos !== word) {
    z = zipfMap.get(noApos);
    if (z) return z;
  }

  // 5) no baseline → skip
  return null;
}



// ---- tokenization helpers ----
function normalizeApos(s){ return s.replace(/[’‘ʼ]/g, "'"); }
function wordsOnlyLower(s){
  const txt = normalizeApos(s.toLowerCase());
  const toks = txt.match(/[a-z]+(?:'[a-z]+)?/g) || [];
  return toks;
}
function alphaTokens(tokens){ return tokens.filter(t => /^[a-z]+(?:'[a-z]+)?$/.test(t)); }

// ---- Zipf from SUBTLEX counts: Zipf = log10(count/N) + 9 ----
const SUBTLEX_N = 51_000_000;
const ZIPF_K = Math.log10(1e9) - Math.log10(SUBTLEX_N); // ≈ 1.292

let zipfMap = new Map();           // word -> Zipf
let humanBigramFreq = new Map();   // "w1 w2" -> freq in human baseline (normalized)
let humanTrigramFreq = new Map();  // "w1 w2 w3" -> freq in human baseline (normalized)
let slopWords = new Set();         // unigrams
let slopBigrams = new Set();       // "w1 w2"
let slopTrigrams = new Set();      // "w1 w2 w3"
let ready = false;

// Subtlex loader with structure fallback
async function loadSUBTLEX() {
  // Primary source
  const url = "https://cdn.jsdelivr.net/npm/subtlex-word-frequencies/index.json";
  const res = await fetch(url);
  if (!res.ok) throw new Error(`SUBTLEX fetch failed ${res.status}`);
  const data = await res.json();

  // Accept formats:
  // 1) Array of {word, count}
  // 2) Object { word: count, ... }
  // 3) Array of [word, count]
  if (Array.isArray(data)) {
    for (const row of data) {
      if (Array.isArray(row) && row.length >= 2) {
        const w = String(row[0]).toLowerCase();
        const c = Number(row[1]) || 0;
        if (c > 0) zipfMap.set(w, Math.log10(c) + ZIPF_K);
      } else if (row && typeof row === 'object' && 'word' in row && 'count' in row) {
        const w = String(row.word).toLowerCase();
        const c = Number(row.count) || 0;
        if (c > 0) zipfMap.set(w, Math.log10(c) + ZIPF_K);
      }
    }
  } else if (data && typeof data === 'object') {
    for (const [w, c] of Object.entries(data)) {
      const cnt = Number(c) || 0;
      if (cnt > 0) zipfMap.set(w.toLowerCase(), Math.log10(cnt) + ZIPF_K);
    }
  }
  if (zipfMap.size === 0) throw new Error("SUBTLEX parsed but empty.");
}

async function loadHumanProfile() {
  const res = await fetch("./human_writing_profile.json");
  if (!res.ok) throw new Error("human_writing_profile.json missing");
  const j = await res.json();
  const hp = j["human-authored"] || j["human"] || j;

  // Expect arrays of objects with {ngram, frequency}
  function norm(list, targetMap) {
    if (!Array.isArray(list)) return;
    let total = 0;
    for (const it of list) {
      const f = Number(it.frequency) || 0;
      total += f;
    }
    if (total <= 0) return;
    for (const it of list) {
      const toks = String(it.ngram || "").toLowerCase().match(/[a-z]+/g);
      if (!toks || toks.length < 2) continue;
      targetMap.set(toks.join(" "), (Number(it.frequency)||0)/total);
    }
  }
  norm(hp.top_bigrams || hp.bigrams || [], humanBigramFreq);
  norm(hp.top_trigrams || hp.trigrams || [], humanTrigramFreq);
}

async function loadSlopSets() {
  // Each file: array like [["word"], ["two words"], ...]
  async function loadSet(path, outSet) {
    const r = await fetch(path);
    if (!r.ok) return; // optional
    const a = await r.json();
    if (!Array.isArray(a)) return;
    for (const item of a) {
      if (!item || !item.length) continue;
      const phrase = String(item[0]).toLowerCase().match(/[a-z]+(?:\s+[a-z]+)*/g);
      if (phrase) outSet.add(phrase[0]);
    }
  }
  await loadSet("./slop_list.json", slopWords);
  await loadSet("./slop_list_bigrams.json", slopBigrams);
  await loadSet("./slop_list_trigrams.json", slopTrigrams);
}

function topKEntries(objOrMap, k, keyTransform = x=>x) {
  const arr = Array.isArray(objOrMap) ? objOrMap.slice()
    : objOrMap instanceof Map ? Array.from(objOrMap.entries())
    : Object.entries(objOrMap);
  arr.sort((a,b)=>b[1]-a[1]);
  return arr.slice(0,k).map(([w,v])=>[keyTransform(w),v]);
}

function renderPills(targetEl, items, fmt=(w,v)=>`<b>${w}</b> <span class="muted">(${v})</span>`) {
  targetEl.innerHTML = items.map(([w,v])=>`<span class="pill">${fmt(w,v)}</span>`).join("");
}

// ----- metrics -----

// Slop index: count hits (unigram + bigram + trigram) over tokens; weights 1,2,8; normalize per 1k words.
function computeSlopIndex(tokens) {
  const n = tokens.length || 0;
  if (!n) return 0;

  let wordHits = 0, biHits = 0, triHits = 0;

  // unigram hits
  if (slopWords.size) {
    for (const t of tokens) if (slopWords.has(t)) wordHits++;
  }

  // bigram hits
  if (slopBigrams.size && n >= 2) {
    for (let i=0;i<n-1;i++){
      const bg = tokens[i] + " " + tokens[i+1];
      if (slopBigrams.has(bg)) biHits++;
    }
  }

  // trigram hits
  if (slopTrigrams.size && n >= 3) {
    for (let i=0;i<n-2;i++){
      const tg = tokens[i] + " " + tokens[i+1] + " " + tokens[i+2];
      if (slopTrigrams.has(tg)) triHits++;
    }
  }

  const totalScore = wordHits + 2*biHits + 8*triHits;
  return (totalScore / n) * 1000;
}

function countItems(arr){
  const m = new Map();
  for (const x of arr) m.set(x, (m.get(x)||0)+1);
  return m;
}

// Like rankOveruse, but returns [ngram, ratio, count] and uses human baseline
function rankOveruseWithCounts(ngrams, humanFreqMap, topK=40) {
  if (!ngrams.length) return [];
  const counts = countItems(ngrams);
  const total = Array.from(counts.values()).reduce((a,b)=>a+b,0) || 1;

  // minimum nonzero human freq
  let minHuman = Infinity;
  for (const v of humanFreqMap.values()) if (v>0 && v<minHuman) minHuman = v;
  if (!isFinite(minHuman)) minHuman = 1e-12;

  const rows = [];
  for (const [ng,cnt] of counts.entries()) {
    const model_f = cnt / total;
    const human_f = humanFreqMap.get(ng) ?? minHuman;
    const ratio   = model_f / (human_f + 1e-12);
    rows.push([ng, ratio, cnt]);
  }
  rows.sort((a,b)=>b[1]-a[1]);          // by over-use ratio
  return rows.slice(0, topK);           // keep top-K
}


// n-grams from tokens without stopwords, alpha only
function contentTokens(tokens){
  return tokens.filter(t => /^[a-z]+(?:'[a-z]+)?$/.test(t) && !STOPWORDS.has(t));
}
function makeNgrams(tokens, n){
  const out = [];
  for (let i=0;i<=tokens.length-n;i++){
    out.push(tokens.slice(i,i+n).join(" "));
  }
  return out;
}

// Over-use vs human baseline: model_freq / human_freq (fallback to minHumanFreq)
function rankOveruse(ngrams, humanFreqMap, topK=50) {
  if (ngrams.length === 0) return [];
  const counts = new Map();
  for (const ng of ngrams) counts.set(ng, (counts.get(ng)||0)+1);
  const total = Array.from(counts.values()).reduce((a,b)=>a+b,0) || 1;

  // minimum nonzero human freq
  let minHuman = Infinity;
  for (const v of humanFreqMap.values()) if (v>0 && v<minHuman) minHuman = v;
  if (!isFinite(minHuman)) minHuman = 1e-12;

  const ratios = [];
  for (const [ng,cnt] of counts.entries()) {
    const model_f = cnt / total;
    const human_f = humanFreqMap.get(ng) ?? minHuman;
    const ratio = model_f / (human_f + 1e-12);
    ratios.push([ng, {ratio, cnt}]);
  }
  ratios.sort((a,b)=>b[1].ratio - a[1].ratio);
  const top = ratios.slice(0, topK);
  return top.map(([ng, v])=>[ng, Number(v.ratio.toFixed(2)), v.cnt]);
}

// Extract exact repeated phrases that match the top trigrams (by over-use ranking).
function extractRepeatedPhrases(text, trigramList, maxOut=1000) {
  if (!trigramList.length) return [];
  const phrases = new Map(); // exact substring -> count
  const lx = text;
  for (const [tg] of trigramList.slice(0, 300)) { // search top 300 trigrams
    const rx = new RegExp(`\\b${tg.replace(/\s+/g,'\\s+')}\\b`, "gi");
    const matches = lx.match(rx);
    if (matches && matches.length) {
      const exacts = matches.map(m => m.trim());
      for (const ex of exacts) phrases.set(ex, (phrases.get(ex)||0)+1);
    }
  }
  const arr = Array.from(phrases.entries()).sort((a,b)=>b[1]-a[1]).slice(0, maxOut);
  return arr;
}

// ---- run ----
async function init() {
  try {
    await loadSUBTLEX();
    await Promise.all([loadHumanProfile(), loadSlopSets()]);
    ready = true;
    ui.status.textContent = "ready";
    ui.btn.disabled = false;
  } catch (e) {
    ui.status.textContent = "error loading resources";
    ui.errors.textContent = String(e?.message || e);
  }
}
init();

ui.btn.addEventListener('click', () => {
  if (!ready) return;
  ui.errors.textContent = "";

  const raw = ui.input.value || "";
  const chars = raw.length;
  const toks0 = wordsOnlyLower(raw);
  const toks = alphaTokens(toks0);
  const nWords = toks.length;

  ui.m_chars.textContent = chars.toLocaleString();
  ui.m_words.textContent = nWords.toLocaleString();

  // Slop
  const slop = computeSlopIndex(toks);
  ui.m_slop.textContent = slop.toFixed(2);

  // Repetition score (bigram+trigram, human-baseline over-use) per your formula
  const toksContent = contentTokens(toks);
  const bigs = makeNgrams(toksContent, 2);
  const tris = makeNgrams(toksContent, 3);

  // rank by over-use vs human; take top 40 of each and sum raw counts
  const topBCounts = rankOveruseWithCounts(bigs, humanBigramFreq, 40);   // [ng, ratio, cnt]
  const topTCounts = rankOveruseWithCounts(tris, humanTrigramFreq, 40);

  const top_bigram_count  = topBCounts.reduce((s, r) => s + r[2], 0);
  const top_trigram_count = topTCounts.reduce((s, r) => s + r[2], 0);
  const total_text_length = toks.length;  // words

  const repetition_score =
    total_text_length > 0
      ? ((top_bigram_count + top_trigram_count) / total_text_length) * 1000
      : 0;

  ui.m_rep.textContent = repetition_score.toFixed(4);

  // Top over-represented words (vs Zipf baseline)
  const wordCounts = countItems(toksContent);
  const wordOverrep = [];
  for (const [w, cnt] of wordCounts.entries()) {
    const zipf = lookupZipf(w);
    if (zipf === null) continue; // skip words without baseline
    const expected = Math.pow(10, zipf - 3); // rough expected freq per 1k words
    const actual = (cnt / nWords) * 1000;
    const ratio = actual / (expected + 1e-9);
    if (ratio > 1.5 && cnt >= 2) { // only show words used more than 1.5x expected and at least twice
      wordOverrep.push([w, ratio, cnt]);
    }
  }
  wordOverrep.sort((a,b)=>b[1]-a[1]);
  const topWords = wordOverrep.slice(0, 40);
  renderPills(ui.topWords, topWords.map(([w,ratio,cnt])=>[w, Number(ratio.toFixed(2))]), (w,v)=>`<b>${w}</b> <span class="muted">${v}×</span>`);

  // Top bigrams and trigrams (already calculated as topBCounts/topTCounts)
  renderPills(
    ui.topBigrams,
    topBCounts.map(([w,ratio])=>[w, Number(ratio.toFixed(2))]),
    (w,v)=>`<b>${w}</b> <span class="muted">${v}×</span>`
  );
  renderPills(
    ui.topTrigrams,
    topTCounts.map(([w,ratio])=>[w, Number(ratio.toFixed(2))]),
    (w,v)=>`<b>${w}</b> <span class="muted">${v}×</span>`
  );

  // Use the trigram data for exact repeated phrases
  const topT = topTCounts.map(([ng,ratio,cnt])=>[ng, ratio]);

  // exact phrases mined from top trigrams
  const phrases = extractRepeatedPhrases(raw, topT, 300);
  renderPills(ui.topPhrases, phrases, (w,v)=>`<b>${w}</b> <span class="muted">(${v})</span>`);
});

// Prefill for quick testing
ui.input.value = `The silence wasn't empty. It wasn't absence; it was pressure. Not a pause—but a held breath. 
For the first time in years, the quiet felt shared. The lattice rose, not just as metal, but as intention.`;
</script>
</body>
</html>
